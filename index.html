<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Marcus Dominguez-Kuhne</title>

  <meta name="author" content="Marcus Dominguez-Kuhne">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon1.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Marcus Dominguez-Kuhne</name>
              </p>
	      <p> I am a Data Scientist and Software Engineer at Amazon FireTV. I use
        machine learning for entity matching (also known as pairwise matching)
        on live production systems. The work I do matches different media items
        together (movies, TV episodes, etc.) from different data providers
        (HBO, Netflix, Amazon Prime, etc.) and clusters them for use as a data
        catalog, used by search. The model works to match and cluster tens of
        thousands of media items prioritizing precision. 
        </p>
        <p>
        Currently, using
        random forest model to accomplish this and actively researching LLM
        related methods such as BeRT and others prioritizing vector comparison
        with transformers and attention. 
        </p>

        <p>Previously, I conducted graduate level research for a year as a PhD student at the University of Southern
        California (USC) advised by <a href="https://robotics.usc.edu/~gaurav/">Professsor Gaurav Sukhatme
        </a> in <a href="https://robotics.usc.edu/resl/"> RESL (Robotics Embedded Systems Laboratory)</a>. I worked at the intersection of machine learning, robotics, and computer vision. My research focused on using machine learning techniques such as deep reinforcement learning and imitation learning for robotic applications including perception and manipulation. I was funded by a USC four year fellowship.
	      </p>

	      <p>

	      I completed my Bachelors in Computer Science at the California Institute of Technology (Caltech), where I worked with <a href="http://www.yisongyue.com">Professor Yisong Yue</a> on multi-agent Motion Planning using Imitation Learning. Also I have researched in the <a href="https://autolab.berkeley.edu">UC Berkeley AUTOLab</a> under <a href="https://goldberg.berkeley.edu">Professor Ken Goldberg</a> using machine learning to intelligently search for desired objects in shelf environments. Additionally, I have researched in the <a href="https://ai.stanford.edu"> Stanford Vision and Learning Lab (SVL) </a> under <a href="http://cvgl.stanford.edu/silvio">Professor Silvio Savarese </a> using deep reinforcement learning for object retrieval in bins.
              </p>
              <p style="text-align:center">
                <a href="mailto:mddoming@caltech.edu">Email</a> &nbsp/&nbsp
                <a href="data/General_Resume_2023.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HylrshMAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/marcus_kuhne">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/marcus-dominguez-kuhne-811b7818a/">LinkedIn</a> <!-- &nbsp/&nbsp -->
                <!--- <a href="https://www.calendly.com/marcusdokudoku/">Schedule Appointment</a> --->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Marcus_Kuhne1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Marcus_Kuhne1.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in reinforcement learning, imitation learning,
                and computer vision for general purpose robotic agents focused
                on manipulation. Much of my research has focused on the
                mechanical search problem searching for physical items in bins
                and shelves and how to use robots in dynamic, home settings to
                better assist individuals. <br>

                Additionally, interested in transformers and attention
                for entity matching problems on real world, messy datasets. 
                
              </p>
            </td>
          </tr>

            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>Publications & Preprints</heading>
	      <br>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><img style="width:100% height:100%">
                <img src='images/lax_ray.png' width="180">
              </div>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.11696">
                <papertitle>Mechanical Search on Shelves using Lateral Access X-RAY</papertitle>
              </a>
              <br>
	      Raven Huang*, <b>Marcus Dominguez-Kuhne*</b>, Jeffery Ichnowski, Vishal Satish, Michael Danielczuk, Kate Sanders, Andrew Lee, Anelia Angelova, Vincent Vanhoucke, Ken Goldberg
              <br>
	      * Indicates equal contribution
	      <br>
              <em>IROS (International Conference on Intelligent Robots and Systems) </em>, 2021
	      <br>
              <a href="https://sites.google.com/berkeley.edu/lax-ray/home">project page</a>
	      <body> / </body>
              <a href="https://arxiv.org/abs/2011.11696">arXiv</a>
	      <body> / </body>
	      <a href="https://venturebeat.com/2020/11/26/robotics-researchers-propose-ai-that-locates-items-on-shelves-and-moves-objects-without-tipping-them/"> Venture Beat Article </a>
              <p></p>
	      <p>LAX-RAY (Lateral Access maXimal Reduction of occupancY support Area) is a system to automate mechanical search for occluded objects on shelves. LAX-RAY couples a perception pipeline predicting target object occupancy support distribution with a mechanical search policy to sequentially select occluding objects to push sideways to quickly reveal the target. </p>
            </td>
	    </tbody></table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypernerf_image'><img style="width:100% height:100%">
                <img src='images/visuo_mech_search.png' width="180">
              </div>
            </td>
            <td style="padding:0px;width:75%;vertical-align:middle">
	      <a href="https://arxiv.org/abs/2008.06073">
                <papertitle>Visuomotor Mechanical Search: Learning to Retrieve Target Objects in Clutter</papertitle>
              </a>
              <br>
	      Andrey Kurenkov*, Joseph Taglic*, Rohun Kulkarni, <b>Marcus Dominguez-Kuhne</b>, Animesh Garg, Roberto Martin-Martin, Silvio Savarese
              <br>
	      * Indicates equal contribution
	      <br>
              <em>IROS (International Conference on Intelligent Robots and Systems) </em>, 2020
	      <br>
              <a href="https://ai.stanford.edu/mech-search/visuomotor/">project page</a>
	      <body> / </body>
              <a href="https://arxiv.org/abs/2008.06073">arXiv</a>
              <p></p>
	      <p> In this work we present a Deep Reinforcement Learning procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our appraoch, trains faster and converges to more efficient uncovering solutions than baselines and ablations, and our policies lead to an average improvement for target graspability. </p>
            </td>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
	      <p> Teaching Assistant: Caltech CS/EE 155 [2020], Machine Learning/Data Mining </p>
	      <p> Teaching Assistant: Caltech CS/EE 155 [2019], Machine Learning/Data Mining </p>
	      <p> Teaching Assistant: Caltech CS/EE 156a [2019], Learning Systems </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
	      <p> Reviewer: IROS 2021
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
